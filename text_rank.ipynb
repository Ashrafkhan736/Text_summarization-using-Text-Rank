{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16394e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from heapq import nlargest\n",
    "from typing import List,Dict\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a21072",
   "metadata": {},
   "source": [
    "## Load the data and get the articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b579dbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    After reaching his hotel in the city, RM revea...\n",
       "1    RM aka Kim Namjoon was the first member to joi...\n",
       "2    Billie Eilish's concert was held in Seoul, Sou...\n",
       "3    BTS ARMY y'all would be missing the members a ...\n",
       "4    BTS member Kim Seokjin aka Jin has the capacit...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./news.csv\")\n",
    "df.dropna(axis=0,inplace=True)\n",
    "articles = df[\"content\"]\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf32b29",
   "metadata": {},
   "source": [
    "## Preprocess the articles by removing unnecessary words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3bac7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocessing(articles):\n",
    "    \"\"\"\n",
    "    This function takes in a pandas Series of articles as input and performs several preprocessing steps to clean and prepare the articles for further analysis.\n",
    "    The steps include:\n",
    "    \n",
    "     1.   Converting all the articles to lowercase.\n",
    "     2.   Removing URLs from the articles using regular expression.\n",
    "     3.   Saving a copy of the original articles for sentence tokenization.\n",
    "     4.   Removing trailing whitespaces from the articles.\n",
    "     5.   Removing punctuations from the articles.\n",
    "     6.  Removing stopwords from the articles.\n",
    "    \n",
    "    The function returns a pandas Series of preprocessed articles.\n",
    "    \n",
    "    Parameters:\n",
    "    articles : pandas Series\n",
    "    A series of articles in string format\n",
    "    \n",
    "    Returns:\n",
    "    pandas Series\n",
    "    A series of preprocessed articles\n",
    "    \"\"\"\n",
    "    global article_original\n",
    "    \n",
    "    #Converting all the artical in lower case\n",
    "    articles = articles.str.lower()\n",
    "    \n",
    "    # remove url\n",
    "    url_pattern = r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "    articles = articles.apply(lambda x : re.sub(url_pattern,\"\" ,x))\n",
    "    \n",
    "    articles = articles.apply(lambda x : re.sub(r'\\n+',\". \",x))\n",
    "    \n",
    "    # Removing the '\\xa0'\n",
    "    articles = articles.apply(lambda x : re.sub(r'\\xa0',' ', x))\n",
    "    \n",
    "    #saving articles (pandas Series) for sentence tokenization\n",
    "    article_original = articles.copy()\n",
    "    \n",
    "    #removing trailing whitespaces\n",
    "    articles = articles.apply(lambda x : re.sub(r' +',' ', x))\n",
    "    \n",
    "    \n",
    "    #removing punctuations\n",
    "    articles = articles.apply(lambda x : ''.join(char for char in x if char not in punctuation))\n",
    "    \n",
    "    #removing trailing whitespaces again\n",
    "    articles = articles.apply(lambda x : re.sub(r' +',' ', x))\n",
    "    \n",
    "    #removing the stopword\n",
    "    articles = articles.apply(lambda x : ' '.join(word for word in x.split() if word not in stop_words))\n",
    "    \n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4ae38",
   "metadata": {},
   "source": [
    "## Creating word frequency for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b21b1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_frequency(article_li: List[Dict[str, float]]) -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    article_li (List[Dict[str, int]]): list of dictionaries, where each dictionary contains a word and its corresponding frequency in an article.\n",
    "    \n",
    "    Returns:\n",
    "    article_li (List[Dict[str, float]]): list of dictionaries, where each dictionary contains a word and its corresponding normalized frequency in an article.\n",
    "    \n",
    "    This function takes in a list of dictionaries where each dictionary contains a word and its corresponding frequency in an article. \n",
    "    The function then iterates over each dictionary, finds the maximum frequency, \n",
    "    and normalizes all the other frequencies by dividing them by the maximum frequency.\n",
    "    The function returns the list of normalized dictionaries.\n",
    "    \"\"\"\n",
    "    for word_freq in article_li:\n",
    "        maxi = max(word_freq.values())\n",
    "        for key in word_freq:\n",
    "            word_freq[key] = word_freq[key]/maxi\n",
    "    return article_li\n",
    "\n",
    "def word_frequency(articles: pd.Series) -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    articles (pd.Series): Pandas series of articles.\n",
    "    \n",
    "    Returns:\n",
    "    norm_freq (List[Dict[str, float]]): list of dictionaries, where each dictionary contains a word and its corresponding normalized frequency in an article.\n",
    "    \n",
    "    This function takes in a list of articles, tokenizes the words in each article, \n",
    "    creates a dictionary for each article containing the words as keys and their frequencies as values, \n",
    "    and appends these dictionaries to the article_li list. \n",
    "    The function then calls the normalize_frequency function to normalize the word frequencies and returns the list of normalized dictionaries.\n",
    "    \n",
    "    \"\"\"\n",
    "    article_li = []\n",
    "    for article in articles:\n",
    "        word_token = word_tokenize(article)\n",
    "        word_freq ={}\n",
    "        for word in word_token:\n",
    "            word_freq[word] = word_freq.get(word,0)+1\n",
    "        article_li.append(word_freq)\n",
    "    norm_freq = normalize_frequency(article_li)\n",
    "    return norm_freq\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d1d91",
   "metadata": {},
   "source": [
    "## Score the sentences in article based on mormalized word frequency for every article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dad535fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_score(norm_freq: List[Dict[str,int]]) -> List[Dict[str,int]]:\n",
    "    \"\"\"\n",
    "    Parameter:\n",
    "    norm_freq : list of dict\n",
    "    each dict contains word as key and their normalized frequency as value\n",
    "    Result:\n",
    "    score_li : list of dict\n",
    "    each dict contains sentence as key and their score as value\n",
    "\n",
    "    This function takes a list of normalized word frequencies and iterates over original articles and their corresponding word frequencies. \n",
    "    It tokenizes sentences and scores them based on the frequency of words present in them, using the provided normalized word frequencies. \n",
    "    The scored sentences are then returned as a list of dictionaries, where the key is the sentence and the value is the score.\n",
    "    \"\"\"\n",
    "    score_li = []\n",
    "    for article,word_freq in zip(article_original,norm_freq):\n",
    "        sentence_token = sent_tokenize(article)\n",
    "        article_score = {}\n",
    "        \n",
    "        # clean all the sentences again\n",
    "        for sentence in sentence_token:\n",
    "            token = \"\".join(char for char in sentence if char not in punctuation)\n",
    "            token = re.sub(r' +',' ',token)\n",
    "            #break all the token in word for scoring sentences\n",
    "            for word in word_tokenize(token):\n",
    "                article_score[sentence] = article_score.get(sentence,0) + word_freq.get(word,0)\n",
    "        score_li.append(article_score)\n",
    "    return score_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ee12e",
   "metadata": {},
   "source": [
    "## summarize the article based on sentence score for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4fd20ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(score_li : List[Dict[str,int]]) -> List[str]:\n",
    "    '''\n",
    "    Parameters:\n",
    "    score_li (List[Dict]): List of dictionaries, where each dictionary contains a sentence and its corresponding score.\n",
    "    \n",
    "    Returns:\n",
    "    List[str]: List of summaries where each summary is a string\n",
    "    \n",
    "    This function takes in a list of dictionaries, where each dictionary contains a sentence and its corresponding score. \n",
    "    The function then iterates over each dictionary, calculates the summary length (25% of the total sentences), \n",
    "    finds the top sentences (based on their scores) using the nlargest() function, and \n",
    "    joins them together using a white space. \n",
    "    The final summary of each article is added to the summary_li list and the function returns this list.\n",
    "    '''\n",
    "    summary_li = []  #initialize an empty list to store summary sentences\n",
    "    for article_score in score_li:\n",
    "        \n",
    "        #calculating the length of summary, assuming 25% of original text as summary\n",
    "        summuary_length = 1 if int(len(article_score) *0.25) <= 1 else int(len(article_score) *0.25) \n",
    "\n",
    "        #getting top sentences from the article_score dictionary\n",
    "        top_sentences = nlargest(summuary_length, article_score, key = article_score.get) \n",
    "        \n",
    "        #joining the top sentences with a dot and append to summary_li\n",
    "        summary_li.append(' '.join(top_sentences)) \n",
    "    \n",
    "    return summary_li  #returning the final summary_li\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9213b4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(articles):\n",
    "    \n",
    "    #clean the article \n",
    "    articles = prepocessing(articles)\n",
    "    \n",
    "    #create normalized word_frequency\n",
    "    norm_frequency = word_frequency(articles)\n",
    "    \n",
    "    #create score for every article\n",
    "    score_li = sentence_score(norm_freq=norm_frequency)\n",
    "    \n",
    "    #create list of summary for every article\n",
    "    summary_li = summary(score_li)\n",
    "    \n",
    "    return summary_li"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaa4b7",
   "metadata": {},
   "source": [
    "## Spliting test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "559fa4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_split(articles,size=0.1):\n",
    "    size = int(size*articles.shape[0])\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.choice(articles.index,size=size)\n",
    "    return articles[idx]\n",
    "test_data = test_train_split(df[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5f6b941c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Content</th>\n",
       "      <th>New Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>BTS members - Taehyung, Suga, RM, Jin, J-Hope,...</td>\n",
       "      <td>there are also rumours of taehyung and suga fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>BTS member J-Hope unveiled his personalised me...</td>\n",
       "      <td>bts member j-hope unveiled his personalised me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>In the over 12 minute-long video, several old ...</td>\n",
       "      <td>\"we all thought workout with bts would be bts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>BTS ARMY is one helluva massive ARMY with a wr...</td>\n",
       "      <td>and we came across some bts armys going crazy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The teaser, titled \"Run BTS! 2022 Special Epis...</td>\n",
       "      <td>2022 special episode - telepathy part 0,\" star...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original Content  \\\n",
       "102  BTS members - Taehyung, Suga, RM, Jin, J-Hope,...   \n",
       "439  BTS member J-Hope unveiled his personalised me...   \n",
       "273  In the over 12 minute-long video, several old ...   \n",
       "106  BTS ARMY is one helluva massive ARMY with a wr...   \n",
       "71   The teaser, titled \"Run BTS! 2022 Special Epis...   \n",
       "\n",
       "                                           New Content  \n",
       "102  there are also rumours of taehyung and suga fl...  \n",
       "439  bts member j-hope unveiled his personalised me...  \n",
       "273  \"we all thought workout with bts would be bts ...  \n",
       "106  and we came across some bts armys going crazy ...  \n",
       "71   2022 special episode - telepathy part 0,\" star...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_li = summarize(test_data)\n",
    "final = pd.DataFrame({\"Original Content\":test_data,\"New Content\":summary_li})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125a99d",
   "metadata": {},
   "source": [
    "## finding which sentences are removed from summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1bd00570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_remove():\n",
    "    remove_list = []\n",
    "    for article,summary in zip(final[\"Original Content\"],final[\"New Content\"]):\n",
    "        art_sent  = set(sent.lower() for sent in sent_tokenize(article))\n",
    "        summ_sent = set(sent_tokenize(summary))\n",
    "        art_sent.difference_update(summ_sent)   \n",
    "        remove_list.append(\" \".join(art_sent))\n",
    "    return remove_list\n",
    "final[\"Removed Lines\"] =sent_remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "424adf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Content</th>\n",
       "      <th>New Content</th>\n",
       "      <th>Removed Lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>BTS members - Taehyung, Suga, RM, Jin, J-Hope,...</td>\n",
       "      <td>there are also rumours of taehyung and suga fl...</td>\n",
       "      <td>both suga and taehyung are seen feeding each o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>BTS member J-Hope unveiled his personalised me...</td>\n",
       "      <td>bts member j-hope unveiled his personalised me...</td>\n",
       "      <td>\"there’s never a dull moment with vmin,\" said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>In the over 12 minute-long video, several old ...</td>\n",
       "      <td>\"we all thought workout with bts would be bts ...</td>\n",
       "      <td>bts recently wrapped their four concerts in la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>BTS ARMY is one helluva massive ARMY with a wr...</td>\n",
       "      <td>and we came across some bts armys going crazy ...</td>\n",
       "      <td>in mid-october, big hit music announced that t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The teaser, titled \"Run BTS! 2022 Special Epis...</td>\n",
       "      <td>2022 special episode - telepathy part 0,\" star...</td>\n",
       "      <td>j-hope added to this, \"we've been together for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Original Content  \\\n",
       "102  BTS members - Taehyung, Suga, RM, Jin, J-Hope,...   \n",
       "439  BTS member J-Hope unveiled his personalised me...   \n",
       "273  In the over 12 minute-long video, several old ...   \n",
       "106  BTS ARMY is one helluva massive ARMY with a wr...   \n",
       "71   The teaser, titled \"Run BTS! 2022 Special Epis...   \n",
       "\n",
       "                                           New Content  \\\n",
       "102  there are also rumours of taehyung and suga fl...   \n",
       "439  bts member j-hope unveiled his personalised me...   \n",
       "273  \"we all thought workout with bts would be bts ...   \n",
       "106  and we came across some bts armys going crazy ...   \n",
       "71   2022 special episode - telepathy part 0,\" star...   \n",
       "\n",
       "                                         Removed Lines  \n",
       "102  both suga and taehyung are seen feeding each o...  \n",
       "439  \"there’s never a dull moment with vmin,\" said ...  \n",
       "273  bts recently wrapped their four concerts in la...  \n",
       "106  in mid-october, big hit music announced that t...  \n",
       "71   j-hope added to this, \"we've been together for...  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "81847166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  0.42424242001836554\n",
      "ROUGE-2:  0.1621621584806429\n",
      "ROUGE-L:  0.42424242001836554\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def metric(row):\n",
    "    scores = rouge.get_scores(row[\"New Content\"], row[\"Original Content\"])\n",
    "    return scores[0][\"rouge-1\"][\"f\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b31cb527",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[\"ROUGE-1\"] = final.apply(metric,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "94780412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Content</th>\n",
       "      <th>New Content</th>\n",
       "      <th>Removed Lines</th>\n",
       "      <th>ROUGE-1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>BTS members - Taehyung, Suga, RM, Jin, J-Hope,...</td>\n",
       "      <td>there are also rumours of taehyung and suga fl...</td>\n",
       "      <td>both suga and taehyung are seen feeding each o...</td>\n",
       "      <td>0.448378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>BTS member J-Hope unveiled his personalised me...</td>\n",
       "      <td>bts member j-hope unveiled his personalised me...</td>\n",
       "      <td>\"there’s never a dull moment with vmin,\" said ...</td>\n",
       "      <td>0.784029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>In the over 12 minute-long video, several old ...</td>\n",
       "      <td>\"we all thought workout with bts would be bts ...</td>\n",
       "      <td>bts recently wrapped their four concerts in la...</td>\n",
       "      <td>0.505576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>BTS ARMY is one helluva massive ARMY with a wr...</td>\n",
       "      <td>and we came across some bts armys going crazy ...</td>\n",
       "      <td>in mid-october, big hit music announced that t...</td>\n",
       "      <td>0.522822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>The teaser, titled \"Run BTS! 2022 Special Epis...</td>\n",
       "      <td>2022 special episode - telepathy part 0,\" star...</td>\n",
       "      <td>j-hope added to this, \"we've been together for...</td>\n",
       "      <td>0.549828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Original Content  \\\n",
       "Index                                                      \n",
       "102    BTS members - Taehyung, Suga, RM, Jin, J-Hope,...   \n",
       "439    BTS member J-Hope unveiled his personalised me...   \n",
       "273    In the over 12 minute-long video, several old ...   \n",
       "106    BTS ARMY is one helluva massive ARMY with a wr...   \n",
       "71     The teaser, titled \"Run BTS! 2022 Special Epis...   \n",
       "\n",
       "                                             New Content  \\\n",
       "Index                                                      \n",
       "102    there are also rumours of taehyung and suga fl...   \n",
       "439    bts member j-hope unveiled his personalised me...   \n",
       "273    \"we all thought workout with bts would be bts ...   \n",
       "106    and we came across some bts armys going crazy ...   \n",
       "71     2022 special episode - telepathy part 0,\" star...   \n",
       "\n",
       "                                           Removed Lines   ROUGE-1  \n",
       "Index                                                               \n",
       "102    both suga and taehyung are seen feeding each o...  0.448378  \n",
       "439    \"there’s never a dull moment with vmin,\" said ...  0.784029  \n",
       "273    bts recently wrapped their four concerts in la...  0.505576  \n",
       "106    in mid-october, big hit music announced that t...  0.522822  \n",
       "71     j-hope added to this, \"we've been together for...  0.549828  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.rename_axis(\"Index\", axis=0, inplace=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c275806",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"./result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2323bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
